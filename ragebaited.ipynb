{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJsS41bQDx_N"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import gc\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import pipeline\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Start timing\n",
        "start_time = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download dataset (run once in Colab)\n",
        "import os\n",
        "if not os.path.exists('/content/training.1600000.processed.noemoticon.csv'):\n",
        "    os.system('pip install kaggle -q')\n",
        "    os.system('mkdir ~/.kaggle && mv /content/kaggle.json ~/.kaggle/')  # Upload kaggle.json manually first\n",
        "    os.system('kaggle datasets download -d kazanova/sentiment140 -p /content/ --unzip -q')\n",
        "\n",
        "# Load dataset\n",
        "csv_path = '/content/training.1600000.processed.noemoticon.csv'\n",
        "df = pd.read_csv(csv_path, encoding='latin-1', header=None,\n",
        "                 names=['sentiment', 'id', 'date', 'query', 'user', 'text'],\n",
        "                 on_bad_lines='skip', engine='python')\n",
        "df = df[['text', 'sentiment']].sample(300, random_state=42).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "DU6F2IY1EUdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Demo metadata\n",
        "df['timestamp'] = pd.to_datetime('now')\n",
        "df['city'] = np.random.choice(['Lahore', 'New York', 'London', 'Tokyo'], size=len(df))\n",
        "df['country'] = np.random.choice(['Pakistan', 'USA', 'UK', 'Japan'], size=len(df))\n",
        "df['age_group'] = np.random.choice(['18-24', '25-34', '35-44'], size=len(df))\n",
        "\n",
        "# embeddings are optional"
      ],
      "metadata": {
        "id": "2DGwKKDQERnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Emotion classifier ( lighter model, no embeddings)\n",
        "emotion_model = pipeline(\n",
        "    'zero-shot-classification',\n",
        "    model='facebook/bart-large-mnli',\n",
        "    device=0 if torch.cuda.is_available() else -1\n",
        ")\n",
        "\n",
        "emotion_to_brain = {\n",
        "    'anger': 'amygdala', 'fear': 'amygdala', 'sadness': 'insula',\n",
        "    'joy': 'nucleus accumbens', 'disgust': 'insula', 'surprise': 'prefrontal cortex'\n",
        "}\n",
        "\n",
        "def score_post_batch(texts):\n",
        "    emotions_list = emotion_model(\n",
        "        texts,\n",
        "        candidate_labels=['anger', 'fear', 'sadness', 'joy', 'disgust', 'surprise'],\n",
        "        multi_label=True,\n",
        "        batch_size=256  # Optimized batch size\n",
        "    )\n",
        "    results = []\n",
        "    for emotions in emotions_list:\n",
        "        emo_dict = dict(zip(emotions['labels'], emotions['scores']))\n",
        "        rage_score = min((emo_dict.get('anger', 0) + emo_dict.get('disgust', 0)), 1.0)\n",
        "        arousal = (emo_dict.get('anger', 0) + emo_dict.get('fear', 0) + emo_dict.get('surprise', 0)) / 3\n",
        "        brain_region = emotion_to_brain[max(emo_dict, key=emo_dict.get)] if max(emo_dict.values()) > 0.5 else None\n",
        "        results.append({'rage_score': rage_score, 'emotions': emo_dict, 'arousal': arousal, 'brain_region': brain_region})\n",
        "    return results"
      ],
      "metadata": {
        "id": "tUbKrlX3EO33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply batch processing\n",
        "batch_size = 256\n",
        "scores_list = []\n",
        "for i in range(0, len(df), batch_size):\n",
        "    batch_texts = df['text'].iloc[i:i + batch_size].tolist()\n",
        "    scores_list.extend(score_post_batch(batch_texts))\n",
        "\n",
        "df['scores'] = scores_list\n",
        "\n",
        "def generate_campaign(row):\n",
        "    if row['scores'].get('rage_score', 0) < 0.3 or row['scores'].get('emotions', {}).get('joy', 0) > 0.5:\n",
        "        emo = max(row['scores'].get('emotions', {}), key=row['scores'].get('emotions', {}).get) if row['scores'].get('emotions', {}) else 'neutral'\n",
        "        return f\"Target {row['city']}, {row['country']} with {emo}-driven campaign for {row['age_group']}.\"\n",
        "    return \"Avoid: High rage potential.\"\n",
        "\n",
        "df['campaign'] = df.apply(generate_campaign, axis=1)"
      ],
      "metadata": {
        "id": "dpMC-X4-EIV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "true_rage = [1 if s['emotions']['anger'] > 0.5 else 0 for s in df['scores'].sample(50, random_state=42)]\n",
        "pred_rage = [1 if s['rage_score'] > 0.5 else 0 for s in df['scores'].sample(50, random_state=42)]\n",
        "f1 = f1_score(true_rage, pred_rage)\n",
        "precision = precision_score(true_rage, pred_rage)\n",
        "recall = recall_score(true_rage, pred_rage)\n",
        "cm = confusion_matrix(true_rage, pred_rage)"
      ],
      "metadata": {
        "id": "a-X2rM2fED33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Emotion distribution for graphing\n",
        "emotion_df = pd.DataFrame(df['scores'].tolist())\n",
        "\n",
        "# Extract emotion scores from the 'emotions' dictionary\n",
        "emotion_scores_extracted = pd.DataFrame(emotion_df['emotions'].tolist())\n",
        "\n",
        "# Melt the extracted emotion scores\n",
        "emotion_scores_melted = emotion_scores_extracted.melt(var_name='emotion', value_name='score')\n",
        "\n",
        "# Group by emotion and calculate the mean score\n",
        "emotion_scores_df = emotion_scores_melted.groupby('emotion')['score'].mean().reset_index()"
      ],
      "metadata": {
        "id": "G-hfSNA_EAtm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b79cd286"
      },
      "source": [
        "!pip install faiss-cpu -q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9411042"
      },
      "source": [
        "!pip install streamlit -q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46b9669c"
      },
      "source": [
        "# %%writefile app.py       // optional streamlit app\n",
        "# import streamlit as st\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "# import time\n",
        "# from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
        "\n",
        "# # Assuming these variables are defined in your Colab session before running this script\n",
        "# # df, f1, precision, recall, start_time, emotion_scores_df, cm\n",
        "\n",
        "# st.title(\"BaitModulus Dashboard\")\n",
        "# st.subheader(\"Metrics\")\n",
        "# st.write(f\"F1 Score (Rage Detection): {f1:.2f}\")\n",
        "# st.write(f\"Precision: {precision:.2f}\")\n",
        "# st.write(f\"Recall: {recall:.2f}\")\n",
        "# st.write(f\"Runtime: {(time.time() - start_time):.2f} seconds\")\n",
        "\n",
        "# st.subheader(\"Emotion Distribution\")\n",
        "# fig1, ax1 = plt.subplots()\n",
        "# sns.barplot(x='emotion', y='score', data=emotion_scores_df, ax=ax1)\n",
        "# ax1.set_title('Average Emotion Scores')\n",
        "# st.pyplot(fig1)\n",
        "\n",
        "# st.subheader(\"Confusion Matrix\")\n",
        "# fig2, ax2 = plt.subplots()\n",
        "# sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax2)\n",
        "# ax2.set_title('Rage Prediction Confusion Matrix')\n",
        "# ax2.set_xlabel('Predicted')\n",
        "# ax2.set_ylabel('True')\n",
        "# st.pyplot(fig2)\n",
        "\n",
        "# st.dataframe(df[['text', 'city', 'country', 'age_group', 'scores', 'campaign']].head())\n",
        "# st.line_chart(df.set_index('timestamp')['scores'].apply(lambda x: x['rage_score']).head(100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48d04c68"
      },
      "source": [
        "print(\"Accuracy Scores:\")\n",
        "print(f\"F1 Score (Rage Detection): {f1:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0f2018f"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Emotion Distribution Graph\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.barplot(x='emotion', y='score', data=emotion_scores_df)\n",
        "plt.title('Average Emotion Scores')\n",
        "plt.ylabel('Average Score')\n",
        "plt.xlabel('Emotion')\n",
        "plt.show()\n",
        "\n",
        "# Confusion Matrix Graph\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Rage Prediction Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "751083d1"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Calculate Accuracy\n",
        "accuracy = accuracy_score(true_rage, pred_rage)\n",
        "\n",
        "# Calculate Specificity (True Negative Rate)\n",
        "# Specificity = True Negatives / (True Negatives + False Positives)\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "\n",
        "print(\"\\nAdditional Metrics:\")\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Specificity: {specificity:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3d2e7a3"
      },
      "source": [
        "# Task\n",
        "Calculate and display accuracy metrics (F1, Precision, Recall, Accuracy, Specificity) for the model's predictions, segmented by 'city', 'country', and 'age_group'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "052b93fd"
      },
      "source": [
        "## Prepare data for segmented analysis\n",
        "\n",
        "### Subtask:\n",
        "Create a new DataFrame that includes the true rage labels and predicted rage labels along with the demographic columns (city, country, age_group).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5005f069"
      },
      "source": [
        "**Reasoning**:\n",
        "Create new columns for true and predicted rage in the dataframe and then create a new dataframe with the demographic columns and the new rage columns.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89971ca8"
      },
      "source": [
        "df['true_rage'] = df['scores'].apply(lambda x: 1 if x['emotions']['anger'] > 0.5 else 0)\n",
        "df['pred_rage'] = df['scores'].apply(lambda x: 1 if x['rage_score'] > 0.5 else 0)\n",
        "df_segmented_metrics = df[['city', 'country', 'age_group', 'true_rage', 'pred_rage']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b67d855"
      },
      "source": [
        "## Calculate metrics by city\n",
        "\n",
        "### Subtask:\n",
        "Group the data by 'city' and calculate the accuracy metrics (F1, Precision, Recall, Accuracy, Specificity) for each city.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c78f93cf"
      },
      "source": [
        "**Reasoning**:\n",
        "Group the DataFrame by city and calculate the specified metrics for each group.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7f2b24f1"
      },
      "source": [
        "city_metrics = {}\n",
        "\n",
        "for city, group in df_segmented_metrics.groupby('city'):\n",
        "    true_rage_city = group['true_rage']\n",
        "    pred_rage_city = group['pred_rage']\n",
        "\n",
        "    if len(true_rage_city) > 0:\n",
        "        f1_city = f1_score(true_rage_city, pred_rage_city)\n",
        "        precision_city = precision_score(true_rage_city, pred_rage_city, zero_division=0)\n",
        "        recall_city = recall_score(true_rage_city, pred_rage_city, zero_division=0)\n",
        "        accuracy_city = accuracy_score(true_rage_city, pred_rage_city)\n",
        "\n",
        "        # Calculate Specificity\n",
        "        cm_city = confusion_matrix(true_rage_city, pred_rage_city)\n",
        "        tn, fp, fn, tp = cm_city.ravel()\n",
        "        specificity_city = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "\n",
        "        city_metrics[city] = {\n",
        "            'F1 Score': f1_city,\n",
        "            'Precision': precision_city,\n",
        "            'Recall': recall_city,\n",
        "            'Accuracy': accuracy_city,\n",
        "            'Specificity': specificity_city\n",
        "        }\n",
        "    else:\n",
        "        city_metrics[city] = {\n",
        "            'F1 Score': np.nan,\n",
        "            'Precision': np.nan,\n",
        "            'Recall': np.nan,\n",
        "            'Accuracy': np.nan,\n",
        "            'Specificity': np.nan\n",
        "        }\n",
        "\n",
        "for city, metrics in city_metrics.items():\n",
        "    print(f\"\\nMetrics for {city}:\")\n",
        "    for metric, value in metrics.items():\n",
        "        print(f\"{metric}: {value:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73f90ada"
      },
      "source": [
        "## Calculate metrics by country\n",
        "\n",
        "### Subtask:\n",
        "Group the data by 'country' and calculate the accuracy metrics for each country.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a10ec3d"
      },
      "source": [
        "**Reasoning**:\n",
        "Group the data by 'country' and calculate the accuracy metrics for each country.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "265d4f6e"
      },
      "source": [
        "country_metrics = {}\n",
        "\n",
        "for country, group in df_segmented_metrics.groupby('country'):\n",
        "    true_rage_country = group['true_rage']\n",
        "    pred_rage_country = group['pred_rage']\n",
        "\n",
        "    if len(true_rage_country) > 0:\n",
        "        f1_country = f1_score(true_rage_country, pred_rage_country, zero_division=0)\n",
        "        precision_country = precision_score(true_rage_country, pred_rage_country, zero_division=0)\n",
        "        recall_country = recall_score(true_rage_country, pred_rage_country, zero_division=0)\n",
        "        accuracy_country = accuracy_score(true_rage_country, pred_rage_country)\n",
        "\n",
        "        # Calculate Specificity\n",
        "        cm_country = confusion_matrix(true_rage_country, pred_rage_country)\n",
        "        tn, fp, fn, tp = cm_country.ravel()\n",
        "        specificity_country = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "\n",
        "        country_metrics[country] = {\n",
        "            'F1 Score': f1_country,\n",
        "            'Precision': precision_country,\n",
        "            'Recall': recall_country,\n",
        "            'Accuracy': accuracy_country,\n",
        "            'Specificity': specificity_country\n",
        "        }\n",
        "    else:\n",
        "        country_metrics[country] = {\n",
        "            'F1 Score': np.nan,\n",
        "            'Precision': np.nan,\n",
        "            'Recall': np.nan,\n",
        "            'Accuracy': np.nan,\n",
        "            'Specificity': np.nan\n",
        "        }\n",
        "\n",
        "for country, metrics in country_metrics.items():\n",
        "    print(f\"\\nMetrics for {country}:\")\n",
        "    for metric, value in metrics.items():\n",
        "        print(f\"{metric}: {value:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3282c93e"
      },
      "source": [
        "## Calculate metrics by age group\n",
        "\n",
        "### Subtask:\n",
        "Group the data by 'age_group' and calculate the accuracy metrics for each age group.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a283161e"
      },
      "source": [
        "**Reasoning**:\n",
        "Initialize an empty dictionary to store age group metrics and iterate through age groups to calculate metrics.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ef43778"
      },
      "source": [
        "age_group_metrics = {}\n",
        "\n",
        "for age_group, group in df_segmented_metrics.groupby('age_group'):\n",
        "    true_rage_age_group = group['true_rage']\n",
        "    pred_rage_age_group = group['pred_rage']\n",
        "\n",
        "    if len(true_rage_age_group) > 0:\n",
        "        f1_age_group = f1_score(true_rage_age_group, pred_rage_age_group, zero_division=0)\n",
        "        precision_age_group = precision_score(true_rage_age_group, pred_rage_age_group, zero_division=0)\n",
        "        recall_age_group = recall_score(true_rage_age_group, pred_rage_age_group, zero_division=0)\n",
        "        accuracy_age_group = accuracy_score(true_rage_age_group, pred_rage_age_group)\n",
        "\n",
        "        # Calculate Specificity\n",
        "        cm_age_group = confusion_matrix(true_rage_age_group, pred_rage_age_group)\n",
        "        tn, fp, fn, tp = cm_age_group.ravel()\n",
        "        specificity_age_group = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "\n",
        "        age_group_metrics[age_group] = {\n",
        "            'F1 Score': f1_age_group,\n",
        "            'Precision': precision_age_group,\n",
        "            'Recall': recall_age_group,\n",
        "            'Accuracy': accuracy_age_group,\n",
        "            'Specificity': specificity_age_group\n",
        "        }\n",
        "    else:\n",
        "        age_group_metrics[age_group] = {\n",
        "            'F1 Score': np.nan,\n",
        "            'Precision': np.nan,\n",
        "            'Recall': np.nan,\n",
        "            'Accuracy': np.nan,\n",
        "            'Specificity': np.nan\n",
        "        }\n",
        "\n",
        "for age_group, metrics in age_group_metrics.items():\n",
        "    print(f\"\\nMetrics for {age_group}:\")\n",
        "    for metric, value in metrics.items():\n",
        "        print(f\"{metric}: {value:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7a3067b"
      },
      "source": [
        "## Display segmented metrics\n",
        "\n",
        "### Subtask:\n",
        "Print or display the calculated metrics for each segment (city, country, age group).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fa71a89"
      },
      "source": [
        "**Reasoning**:\n",
        "Print the calculated metrics for each segment (city, country, age group) by iterating through the dictionaries and formatting the output.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ed775cb4"
      },
      "source": [
        "print(\"Metrics by City:\")\n",
        "for city, metrics in city_metrics.items():\n",
        "    print(f\"\\n{city}:\")\n",
        "    for metric, value in metrics.items():\n",
        "        print(f\"  {metric}: {value:.2f}\")\n",
        "\n",
        "print(\"\\nMetrics by Country:\")\n",
        "for country, metrics in country_metrics.items():\n",
        "    print(f\"\\n{country}:\")\n",
        "    for metric, value in metrics.items():\n",
        "        print(f\"  {metric}: {value:.2f}\")\n",
        "\n",
        "print(\"\\nMetrics by Age Group:\")\n",
        "for age_group, metrics in age_group_metrics.items():\n",
        "    print(f\"\\n{age_group}:\")\n",
        "    for metric, value in metrics.items():\n",
        "        print(f\"  {metric}: {value:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e04ab29"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   Metrics for each city, country, and age group were successfully calculated and displayed.\n",
        "*   All countries and age groups showed a Recall of 1.00, indicating that all positive instances ('rage') were correctly identified within these segments.\n",
        "*   Performance varied across segments for other metrics. For instance, the UK generally showed higher Accuracy and Specificity compared to other countries.\n",
        "*   The metrics for cities, countries, and age groups are available for review, providing a segmented view of the model's performance.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   Investigate the reasons for the perfect Recall (1.00) across all countries and age groups. This could indicate a potential bias or issue with the dataset or model.\n",
        "*   Analyze the variation in Precision, Accuracy, and Specificity across different cities, countries, and age groups to identify segments where the model performs less optimally and may require further tuning or data collection.\n"
      ]
    }
  ]
}